# File: pqc_inspector_server/services/ollama_service.py
# ğŸ¤– Ollama AI ëª¨ë¸ê³¼ í†µì‹ í•˜ëŠ” ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.

import ollama
from typing import Dict, Any, Optional
from ..core.config import settings

class OllamaService:
    def __init__(self):
        self.base_url = settings.OLLAMA_BASE_URL
        self.client = ollama.Client(host=self.base_url)
        print("OllamaServiceê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    async def generate_response(self, model: str, prompt: str, system_prompt: Optional[str] = None) -> Dict[str, Any]:
        """
        Ollama ëª¨ë¸ì—ê²Œ í”„ë¡¬í”„íŠ¸ë¥¼ ì „ì†¡í•˜ê³  ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤.
        """
        try:
            print(f"ğŸ¤– Ollama ëª¨ë¸ í˜¸ì¶œ ì‹œì‘: {model}")
            print(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} characters")
            
            messages = []
            
            if system_prompt:
                messages.append({
                    "role": "system",
                    "content": system_prompt
                })
                print(f"ğŸ“‹ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(system_prompt)} characters")
            
            messages.append({
                "role": "user", 
                "content": prompt
            })

            import time
            start_time = time.time()
            
            response = self.client.chat(
                model=model,
                messages=messages,
                stream=False
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            print(f"âœ… Ollama ì‘ë‹µ ì™„ë£Œ: {duration:.2f}ì´ˆ")
            print(f"ğŸ“Š ì‘ë‹µ ê¸¸ì´: {len(response['message']['content'])} characters")
            print(f"ğŸ§  í† í° ì‚¬ìš©ëŸ‰ - ì…ë ¥: {response.get('prompt_eval_count', 0)}, ì¶œë ¥: {response.get('eval_count', 0)}")
            
            return {
                "success": True,
                "content": response['message']['content'],
                "model": model,
                "total_duration": response.get('total_duration', 0),
                "load_duration": response.get('load_duration', 0),
                "prompt_eval_count": response.get('prompt_eval_count', 0),
                "eval_count": response.get('eval_count', 0),
                "actual_duration": duration
            }
            
        except Exception as e:
            print(f"âŒ Ollama ëª¨ë¸ '{model}' í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "success": False,
                "error": str(e),
                "content": None
            }

    async def check_model_availability(self, model: str) -> bool:
        """
        ì§€ì •ëœ ëª¨ë¸ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸í•©ë‹ˆë‹¤.
        """
        try:
            models = self.client.list()
            available_models = [m['name'] for m in models['models']]
            return model in available_models
        except Exception as e:
            print(f"ëª¨ë¸ '{model}' í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False

# ì˜ì¡´ì„± ì£¼ì…ì„ ìœ„í•œ í•¨ìˆ˜
def get_ollama_service():
    return OllamaService()